Here is a well-structured **README.md** file for your project:  

---

## **Speech Emotion Recognition (SER) using Machine Learning & Deep Learning**  

### **ğŸ“Œ Project Overview**  
This project aims to classify emotions from speech using **Machine Learning (Decision Tree)** and **Deep Learning (CNN)** models. It utilizes **MFCC features** extracted from **RAVDESS and TESS** datasets to train models for recognizing emotions like **neutral, happy, sad, angry, fearful, disgust, surprised, and calm**.

---

### **ğŸ“‚ Folder Structure**  
```
ğŸ“¦ PROJ
 â”£ ğŸ“‚ data                       # Raw dataset files (if applicable)
 â”£ ğŸ“‚ dataset_features           # Processed dataset stored as joblib files
 â”ƒ â”£ ğŸ“œ X.joblib                 # Feature set (MFCCs)
 â”ƒ â”£ ğŸ“œ y.joblib                 # Labels corresponding to features
 â”£ ğŸ“‚ models                     # Saved trained models
 â”ƒ â”£ ğŸ“œ cnn_model.h5             # Trained CNN model
 â”ƒ â”£ ğŸ“œ decision_tree.pkl        # Trained Decision Tree model
 â”£ ğŸ“‚ src                        # Core source code
 â”ƒ â”£ ğŸ“œ data_loader.py           # Loads dataset features
 â”ƒ â”£ ğŸ“œ evaluation.py            # Evaluates models
 â”ƒ â”£ ğŸ“œ model_saving.py          # Saves and loads models
 â”ƒ â”£ ğŸ“œ model_training.py        # Trains Decision Tree & CNN models
 â”ƒ â”£ ğŸ“œ visualize_results.py     # Plots accuracy/loss graphs
 â”£ ğŸ“œ config.py                  # Project configurations
 â”£ ğŸ“œ create_features.py         # Extracts MFCC features & saves joblib files
 â”£ ğŸ“œ input.py                   # Handles input processing
 â”£ ğŸ“œ main.py                    # Runs the full pipeline (training & evaluation)
 â”£ ğŸ“œ pred.py                    # Predicts emotion from new audio
 â”£ ğŸ“œ requirements.txt           # Required Python libraries
 â”£ ğŸ“œ sample_audio.wav           # Sample audio file for testing
 â”£ ğŸ“œ tess_pipeline_for_data.py  # Prepares dataset folders
 â”— ğŸ“œ README.md                  # Project documentation
```

---

### **ğŸ“Š Dataset Information**  
The dataset consists of **5,252 speech samples** from:  

- **[RAVDESS](https://zenodo.org/record/1188976)**: **2,452 samples** (speech & song) recorded by **24 actors** expressing 8 emotions.  
- **[TESS](https://tspace.library.utoronto.ca/handle/1807/24487)**: **2,800 samples**, recorded by **two actresses** speaking target words in 7 emotions.  

Each sample is converted into **40-dimensional MFCC features** for model training.

---

### **âš™ï¸ Installation & Setup**  
#### **1ï¸âƒ£ Clone the repository**  
```bash
git clone <repo-link>
cd PROJ
```
#### **2ï¸âƒ£ Install dependencies**  
```bash
pip install -r requirements.txt
```
#### **3ï¸âƒ£ Run the main pipeline**  
```bash
python main.py
```
#### **4ï¸âƒ£ Predict emotion from an audio file**  
```bash
python pred.py --file sample_audio.wav
```

---

### **ğŸ–¥ï¸ Model Training & Evaluation**  
- **Decision Tree**: Baseline ML model, **67% accuracy**  
- **CNN**: Deep learning model, **84% accuracy**  

To retrain models:  
```bash
python main.py --train
```

---

### **ğŸ“Š Results & Observations**  
âœ… **CNN outperforms Decision Tree**, learning deep hierarchical speech features.  
âœ… **Best classified emotion:** Neutral (94% Precision).  
âœ… **Most misclassified emotion:** Surprised (79% Recall).  
âœ… **Potential improvements:** Hybrid CNN-LSTM model, dataset augmentation, transformer-based architectures.

---

### **ğŸ“© Contributors**  
- **Bhushan Gunjal**
- **Pravin Patrike**
- **Varsha Rai**

For any questions, feel free to reach out! ğŸš€ğŸ”¥  

---

This **README** is clean, concise, and follows best practices. Let me know if you'd like any modifications! ğŸš€ğŸ“„